{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_DIR = Path('../../Data/Wind Turbine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from source.common import get_sensor_data_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['INPUT_DATA_DIR'] = '/media/dmitriy/D/Projects/PredictiveMaintenance/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First look at data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCADA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scada_df = pd.read_csv(DATA_DIR / 'scada_data.csv')\n",
    "scada_df['DateTime'] = pd.to_datetime(scada_df['DateTime'], format='%m/%d/%Y %H:%M')\n",
    "scada_df['HasError'] = (scada_df['Error'] != 0).astype(int)\n",
    "original_columns = scada_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scada_df['DateTimeR'] = scada_df['DateTime'].dt.round(freq='10min')\n",
    "# scada_df_gr = scada_df.groupby('DateTimeR', as_index=False).mean()\n",
    "\n",
    "date_range = pd.Series(pd.date_range(start=scada_df['DateTimeR'].min(), end=scada_df['DateTimeR'].max(), freq='10min'), name='DateTimeR')\n",
    "scada_df_gr = scada_df.merge(date_range, how='outer', on='DateTimeR').sort_values('DateTimeR')\n",
    "scada_df_gr['HasMissing'] = (scada_df_gr['Time'].isna()).astype(int)\n",
    "\n",
    "date_range_10s = pd.Series(pd.date_range(start=scada_df_gr['DateTimeR'].min(), end=scada_df_gr['DateTimeR'].max(), freq='10s'), name='DateTime10s')\n",
    "scada_df_gr = scada_df_gr.merge(date_range_10s, how='outer', left_on='DateTimeR', right_on='DateTime10s').sort_values('DateTime10s').ffill().reset_index(drop=True)\n",
    "scada_df_gr.loc[scada_df_gr['HasMissing'] == 1, original_columns] = np.nan\n",
    "scada_df_gr.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scada_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tpl for tpl in zip(scada_df['DateTime'], scada_df['Error'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fault_df = pd.read_csv(DATA_DIR / 'fault_data.csv')\n",
    "fault_df['DateTime'] = pd.to_datetime(fault_df['DateTime'], format='%Y-%m-%d %H:%M:%S')\n",
    "fault_df['TimeDiff'] = fault_df['DateTime'] - fault_df['DateTime'].shift(1)\n",
    "fault_df.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fault_df['DateTimeR'] = fault_df['DateTime'].dt.round(freq='10s')\n",
    "\n",
    "grouped_records = []\n",
    "for dt, group_df in fault_df.groupby('DateTimeR', as_index=False):\n",
    "    fault_record = (dt, ','.join(group_df['Fault'].unique()))\n",
    "    grouped_records.append(fault_record)\n",
    "\n",
    "grouped_fault_df = pd.DataFrame.from_records(grouped_records, columns=['DateTime', 'Faults'])\n",
    "\n",
    "for fault_type in fault_df['Fault'].unique():\n",
    "    grouped_fault_df[f'Fault_{fault_type}'] = (grouped_fault_df['Faults'].str.contains(fault_type)).astype(int)\n",
    "\n",
    "# grouped_fault_df.head(50)\n",
    "\n",
    "# date_range = pd.Series(pd.date_range(start=scada_df_gr['DateTimeR'].min(), end=scada_df_gr['DateTimeR'].max(), freq='10min'), name='DateTime')\n",
    "# grouped_fault_df = grouped_fault_df.merge(date_range, how='outer', on='DateTime').sort_values('DateTime')\n",
    "# grouped_fault_df['HasMissing'] = (grouped_fault_df['Faults'].isna()).astype(int)\n",
    "\n",
    "grouped_fault_df['TimeSincePrevFault'] = grouped_fault_df['DateTime'] - grouped_fault_df['DateTime'].shift(1)\n",
    "grouped_fault_df['FaultGroupID'] = (grouped_fault_df['TimeSincePrevFault'] > timedelta(hours=6)).astype(int)\n",
    "grouped_fault_df['FaultGroupID'] = grouped_fault_df['FaultGroupID'].cumsum()\n",
    "grouped_fault_df.head(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fault_df['Fault'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Status data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_df = pd.read_csv(DATA_DIR / 'status_data.csv')\n",
    "status_df['Time'] = pd.to_datetime(status_df['Time'], format='%d/%m/%Y %H:%M:%S')\n",
    "status_df = status_df.rename(columns={'Time': 'DateTime'})\n",
    "status_df['DateTime10s'] = status_df['DateTime'].dt.round(freq='10s')\n",
    "status_df['StatusRecordID'] = status_df.index\n",
    "status_df['StateDurationTD'] = status_df['DateTime'].shift(-1) - status_df['DateTime']\n",
    "status_df['StateDurationMinutes'] = status_df['StateDurationTD'].dt.total_seconds() / 60\n",
    "status_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_df['Status Text'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_df.head(980).tail(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "date_range = pd.Series(pd.date_range(start=status_df['DateTime10s'].min(), end=status_df['DateTime10s'].max(), freq='10s'), name='DateTime10s')\n",
    "status_df_ts = status_df[['DateTime10s', 'Main Status', 'Sub Status', 'StatusRecordID']]\\\n",
    "    .drop_duplicates(subset=['DateTime10s'], keep='last')\\\n",
    "    .merge(date_range, how='outer', on='DateTime10s')\\\n",
    "    .sort_values('DateTime10s')\\\n",
    "    .ffill()\\\n",
    "    .reset_index(drop=True)\n",
    "status_df_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing data before fault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fault_group_id = 3\n",
    "fault_group_id_data = grouped_fault_df[grouped_fault_df['FaultGroupID'] == fault_group_id]\n",
    "fault_time_start = fault_group_id_data['DateTime'].min()\n",
    "fault_time_end = fault_group_id_data['DateTime'].max()\n",
    "fault_group_id_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scada_status_df = scada_df_gr.merge(status_df_ts, on='DateTime10s', how='outer').sort_values('DateTime10s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_to_analyze = scada_status_df[\n",
    "    (scada_status_df['DateTime10s'] >= fault_time_start - timedelta(hours=1))\n",
    "    & (scada_status_df['DateTime10s'] <= fault_time_end + timedelta(hours=1))\n",
    "].copy()\n",
    "\n",
    "data_to_analyze['IsFault'] = 0\n",
    "data_to_analyze.loc[\n",
    "    (data_to_analyze['DateTime10s'] >= fault_time_start)\n",
    "    & (data_to_analyze['DateTime10s'] <= fault_time_end),\n",
    "    'IsFault'\n",
    "] = 1\n",
    "data_to_analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# status_df[(status_df['StatusRecordID'] >= 488) & (status_df['StatusRecordID'] <= 514)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = 3\n",
    "for ifig in range(int(len(data_to_analyze.columns[2:]) / n_cols) + 1):\n",
    "    fig, axes = plt.subplots(figsize=(22, 5), ncols=n_cols)\n",
    "\n",
    "    for iax in range(n_cols):\n",
    "        sns.lineplot(x='DateTime10s', y=data_to_analyze.columns[2 + ifig*n_cols + iax], data=data_to_analyze, hue='IsFault', ax=axes[iax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_df[\n",
    "    status_df['StatusRecordID'].isin(np.arange(604, 612))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = get_sensor_data_info(None)['Column_valid']\n",
    "column_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = datetime(2014, 5, 1)\n",
    "train_end = datetime(2015, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scada_status_df[(scada_status_df['DateTime'].notna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scada_status_df['IsFault'] = 0\n",
    "for fault_id, fault_df in grouped_fault_df.groupby('FaultGroupID'):\n",
    "    fault_time_start = fault_df['DateTime'].min()\n",
    "    fault_time_end = fault_df['DateTime'].max()\n",
    "    \n",
    "    scada_status_df.loc[\n",
    "        (scada_status_df['DateTime10s'] >= fault_time_start)\n",
    "        & (scada_status_df['DateTime10s'] <= fault_time_end),\n",
    "        'IsFault',\n",
    "    ] = 1\n",
    "    \n",
    "    scada_status_df.loc[\n",
    "        (scada_status_df['DateTime10s'] >= fault_time_start)\n",
    "        & (scada_status_df['DateTime10s'] <= fault_time_end),\n",
    "        'FaultGroupID',\n",
    "    ] = fault_id\n",
    "\n",
    "scada_status_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = scada_status_df[\n",
    "    (scada_status_df['DateTime'].notna())\n",
    "    & (scada_status_df['DateTime'] >= train_start)\n",
    "    & (scada_status_df['DateTime'] <= train_end)\n",
    "][['DateTime10s', 'HasMissing', *column_mapping.keys(), 'IsFault', 'FaultGroupID']].copy()\n",
    "train_df = train_df.rename(columns=column_mapping)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    *list(column_mapping.values())[3:],\n",
    "    'HasMissing',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, min_samples_split=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_df[features], train_df['IsFault'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rf_model.pickle', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/rf_model.pickle', 'rb') as file:\n",
    "    model_exp = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_p = model_exp.predict_proba(train_df[features])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_exp.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_exp.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pred_m_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e43c39553c8ece0ebdcdaf937db4c3609304cd7e7c22393415e387d2529d2d49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
